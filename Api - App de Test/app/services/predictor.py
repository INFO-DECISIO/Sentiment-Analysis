import os
import joblib
import pandas as pd
import numpy as np
from flask import current_app


class SentimentPredictor:
    """Service pour charger et utiliser les mod√®les de pr√©diction de sentiment"""

    def __init__(self, app=None):
        """Initialise le service en chargeant les mod√®les"""
        self.text_model = None
        self.text_model_loaded = False
        self.text_model_name = "Unknown"

        self.full_model = None
        self.full_model_loaded = False
        self.full_model_name = "Unknown"

        # Stocke l'app pour une utilisation ult√©rieure si fournie
        self.app = app

        # Les mod√®les seront charg√©s lors de la premi√®re utilisation
        # ou explicitement via load_models()

    def load_models(self, app=None):
        """Charge explicitement les mod√®les, utile pour les initialiser avec l'app"""
        if app:
            self.app = app

        try:
            # Utiliser le contexte de l'application pour charger les mod√®les
            with self.app.app_context():
                self._load_models()
        except Exception as e:
            print(f"Erreur lors du chargement des mod√®les: {str(e)}")

    def _load_models(self):
        """Charge les mod√®les √† partir des fichiers joblib"""
        # Mod√®le texte seulement
        try:
            # Utilise le mod√®le par d√©faut si nous ne sommes pas dans un contexte d'application
            text_model_path = current_app.config.get('MODEL_TEXT_PATH', 'app/models/sentiment_model_text_only.joblib')

            if os.path.exists(text_model_path):
                self.text_model = joblib.load(text_model_path)
                self.text_model_loaded = True
                self.text_model_name = "Multinomial NB (Text Only)"
                print(f"Mod√®le texte charg√© depuis {text_model_path}")
                if hasattr(current_app, 'logger'):
                    current_app.logger.info(f"Mod√®le texte charg√© depuis {text_model_path}")
            else:
                print(f"Fichier mod√®le texte non trouv√©: {text_model_path}")
                if hasattr(current_app, 'logger'):
                    current_app.logger.error(f"Fichier mod√®le texte non trouv√©: {text_model_path}")
        except Exception as e:
            print(f"Erreur lors du chargement du mod√®le texte: {str(e)}")
            if hasattr(current_app, 'logger'):
                current_app.logger.error(f"Erreur lors du chargement du mod√®le texte: {str(e)}")

        # Mod√®le texte + m√©tadonn√©es
        try:
            # Utilise le mod√®le par d√©faut si nous ne sommes pas dans un contexte d'application
            full_model_path = current_app.config.get('MODEL_FULL_PATH',
                                                     'app/models/sentiment_model_text_metadata.joblib')

            if os.path.exists(full_model_path):
                self.full_model = joblib.load(full_model_path)
                self.full_model_loaded = True
                self.full_model_name = "SVC Linear (Text + Metadata)"
                print(f"Mod√®le avec m√©tadonn√©es charg√© depuis {full_model_path}")
                if hasattr(current_app, 'logger'):
                    current_app.logger.info(f"Mod√®le avec m√©tadonn√©es charg√© depuis {full_model_path}")
            else:
                print(f"Fichier mod√®le avec m√©tadonn√©es non trouv√©: {full_model_path}")
                if hasattr(current_app, 'logger'):
                    current_app.logger.error(f"Fichier mod√®le avec m√©tadonn√©es non trouv√©: {full_model_path}")
        except Exception as e:
            print(f"Erreur lors du chargement du mod√®le avec m√©tadonn√©es: {str(e)}")
            if hasattr(current_app, 'logger'):
                current_app.logger.error(f"Erreur lors du chargement du mod√®le avec m√©tadonn√©es: {str(e)}")

    def _format_result(self, prediction_encoded, probability):
        """Formate le r√©sultat de la pr√©diction"""
        # R√©cup√©rer le mapping et le seuil de confiance
        try:
            sentiment_map = current_app.config.get('SENTIMENT_MAP', {
                0: {'label': 'neutral', 'value': 0, 'emoji': 'üòê'},
                1: {'label': 'positive', 'value': 1, 'emoji': 'üòÑ'},
                2: {'label': 'negative', 'value': -1, 'emoji': 'üòî'}
            })
            threshold = current_app.config.get('CONFIDENCE_THRESHOLD', 0.2)
            undetermined = current_app.config.get('UNDETERMINED', {
                'label': 'undetermined', 'value': None, 'emoji': 'ü§î'
            })
        except RuntimeError:
            # Fallback values if not in app context
            sentiment_map = {
                0: {'label': 'neutral', 'value': 0, 'emoji': 'üòê'},
                1: {'label': 'positive', 'value': 1, 'emoji': 'üòÑ'},
                2: {'label': 'negative', 'value': -1, 'emoji': 'üòî'}
            }
            threshold = 0.5
            undetermined = {'label': 'undetermined', 'value': None, 'emoji': 'ü§î'}

        # Si la probabilit√© est trop faible, consid√©rer comme ind√©termin√©
        if probability is not None and probability < threshold:
            result = undetermined
        else:
            # Sinon, obtenir le r√©sultat √† partir du mapping
            result = sentiment_map.get(prediction_encoded, undetermined)

        return result, probability

    def predict_text_only(self, text):
        """Pr√©dit le sentiment √† partir du texte seulement"""
        # Charge les mod√®les si ce n'est pas d√©j√† fait
        if not self.text_model_loaded:
            self._load_models()

        if not self.text_model_loaded:
            raise ValueError("Mod√®le texte non disponible")

        # Pr√©parer les donn√©es d'entr√©e (liste de texte)
        input_data = [text]

        # Faire la pr√©diction
        prediction_encoded = self.text_model.predict(input_data)[0]

        # Obtenir les probabilit√©s si disponibles
        probability = None
        raw_probas = None

        if hasattr(self.text_model, 'predict_proba'):
            raw_probas = self.text_model.predict_proba(input_data)[0].tolist()
            probability = float(raw_probas[prediction_encoded])

        # Formater le r√©sultat
        result, probability = self._format_result(prediction_encoded, probability)

        return result, probability, raw_probas

    def predict_with_metadata(self, cleaned_text, platform, time):
        """Pr√©dit le sentiment avec texte + m√©tadonn√©es"""
        # Charge les mod√®les si ce n'est pas d√©j√† fait
        if not self.full_model_loaded:
            self._load_models()

        if not self.full_model_loaded:
            raise ValueError("Mod√®le avec m√©tadonn√©es non disponible")

        # Pr√©parer les donn√©es d'entr√©e
        input_data = {
            'cleaned_text': cleaned_text,
            'Platform': platform,
            'Time of Tweet': time
        }

        # Convertir en DataFrame
        input_df = pd.DataFrame([input_data])

        # Faire la pr√©diction
        prediction_encoded = self.full_model.predict(input_df)[0]

        # Obtenir les probabilit√©s si disponibles
        probability = None
        raw_probas = None

        if hasattr(self.full_model, 'predict_proba'):
            raw_probas = self.full_model.predict_proba(input_df)[0].tolist()
            probability = float(raw_probas[prediction_encoded])

        # Formater le r√©sultat
        result, probability = self._format_result(prediction_encoded, probability)

        return result, probability, raw_probas